<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2020-06-11T15:50:22+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Synthetic Mind</title><subtitle>Exploring alternatives to deep reinforcement learning.  </subtitle><author><name>Synthetic Mind</name><email>your-email@email.com</email></author><entry><title type="html">TRAINING GAME AGENTS WITH SUPERVISED LEARNING</title><link href="http://localhost:4000/supervisedagents/" rel="alternate" type="text/html" title="TRAINING GAME AGENTS WITH SUPERVISED LEARNING" /><published>2020-06-01T01:00:00+07:00</published><updated>2020-06-01T01:00:00+07:00</updated><id>http://localhost:4000/supervisedagents</id><content type="html" xml:base="http://localhost:4000/supervisedagents/">&lt;iframe width=&quot;600&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0fAOLhMN1n8&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h6 id=&quot;an-agent-trained-with-supervised-learinng-on-the-lunar-lander-environment&quot;&gt;&lt;em&gt;An agent trained with supervised learinng on the Lunar lander environment.&lt;/em&gt;&lt;/h6&gt;

&lt;p&gt;Reinforcement Learning is a general, robust and performant paradigm. Its strengths become evident when combined with deep learning. Deep reinforcement learning can solve increasingly complex tasks with huge state space and complex rules. It has produced world champion-beating artificial agents in the games of chess, DOTA, go, and Starcraft. One may argue that any problem where scores are received for performing actions should be framed as a reinforcement learning problem. But behind these notable successes, lies huge problems. Deep reinforcement learning is hard to work with. A lot of effort goes into making agents learn correctly and robustly. Deep reinforcement learning also needs a huge amount of compute. Hundreds of thousands of dollars in compute is needed to reproduce the most impressive results. A lone researcher with limited resources cannot comfortably go beyond Atari games state space when experimenting with game playing agents.&lt;/p&gt;

&lt;p&gt;Lots of effort has been made to improve the sample efficiency of these deep RL algorithms. Model based reinforcement learning is a promising research direction. The environment dynamics can be mastered using supervised/ self-supervised learning, then smaller representations can be used to learn to maximise cumulative reward. This paradigm is attractive because once you learn representations of the state, you may use any kind of valid algorithm to maximise rewards.&lt;/p&gt;

&lt;p&gt;Another promising paradigm is Upside down RL, where the appropriate action to take in a given state is learned by injecting additional information when learning to map state to actions. In settings where this has worked,  an expected goal, in addition to the state, is used as input. The agent is trained to produce actions that reach the goal. The goal in this case is to gain a given reward in a given number of time steps. Inspiration from this paradigm leads me to this month’s article.&lt;/p&gt;

&lt;p&gt;We know that we can use a neural network to map state to actions and use a reward proxy in our loss function to maximise the probability that the actions taken will yield the highest reward. 
An example of this loss function is the policy gradient loss:&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;supervisedagents/loss.png&quot; alt=&quot;policy gradient loss&quot; /&gt;
&lt;figcaption&gt;Fig 1. Policy gradient loss.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As shown above to get the gradient we not only need the episode trajectories in the actions a(t), predicted by phi given a state s(t),  we also need to have Ø(t) which is a form of the reward function.
Adding this  Ø(t) to our loss function can make the agent harder to train and introduces a host of new hyperparameters to tune. 
Although there have been a number of improvements to replace the Ø(t) function. A valid research question to ask is if we can remove the reward from our loss function and still have an agent perform actions that maximise reward.&lt;/p&gt;

&lt;p&gt;Imitation learning, where a set of human/expert demonstrations are used to train a policy has been successful in training robust game plating agents. But this method needs human/ expert demonstrations in the first place. This is an inefficient and low yield way of collecting data for training a policy.&lt;/p&gt;

&lt;p&gt;A better approach given the constraints, is to find a way that we can collect high quality data for supervised learning without using any kind of demonstrations.  We define high quality data as state action pairs that can be used to improve a suboptimal policy.
In this first article in the series, we explore the use of the reward function to select high quality state action pairs for the model to train on. In this way we get rid of Ø(t) and also show that we can get promising results on toy tasks, while using a much simpler loss function.&lt;/p&gt;

&lt;h4 id=&quot;high-quality-episode-selection-using-a-reward-heuristic&quot;&gt;HIGH QUALITY EPISODE SELECTION USING A REWARD HEURISTIC&lt;/h4&gt;

&lt;p&gt;The simplest choice of a heuristic for obtaining high quality data is using the top K rewards . This data that can be collected by performing probabilistic actions with our current policy and sorting the episodes by rewards from highest to lowest. We can then randomly pick state action pairs from the top K episodes and use them to train a neural network.&lt;/p&gt;

&lt;p&gt;To explain this method step by step:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Initialise a neural network &lt;strong&gt;f(x)&lt;/strong&gt; that takes states as input and outputs actions&lt;/li&gt;
  &lt;li&gt;Collect data by using &lt;strong&gt;f(x)&lt;/strong&gt; to collect state-action pairs for N number of episodes.&lt;/li&gt;
  &lt;li&gt;Sort the episode data by reward from highest to lowest.&lt;/li&gt;
  &lt;li&gt;Select top K episodes and sample state action action pairs from these&lt;/li&gt;
  &lt;li&gt;Train &lt;strong&gt;f(x)&lt;/strong&gt; with these state action pairs&lt;/li&gt;
  &lt;li&gt;Repeat from 2 till reward is satisfactory.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;

&lt;p&gt;We use the above method to maximise rewards on two open ai gym tasks: CartPole and Lunar Lander.
CartPole Results
The model can consistently get to the top score of 200 in the environment&lt;/p&gt;

&lt;p&gt;Batch size = 256; max timesteps = 70; updates per iter: 100; episodes sampled per iter: 150&lt;/p&gt;

&lt;p&gt;Lunar Lander Results
However, the same fixed hyperparameters do not work in the LunarLander environment.&lt;/p&gt;

&lt;p&gt;Run1; random seed; Batch size = 256; Total Rew Possible: &amp;gt;200&lt;/p&gt;

&lt;p&gt;Increasing sample size aka batch size achieves significantly higher scores but is still unstable.
Run1 seed = 46; batch size = 2048;&lt;/p&gt;

&lt;p&gt;The fact that we can get good results by choosing top K training episodes and iteratively training on its state action pairs, merits further investigation. There may exist other methods of getting high quality data and this is the main focus of this continuing research.&lt;/p&gt;

&lt;p&gt;The next article in this series will highlight efforts in making the algorithm more stable, data selection, adding more complex test games and conducting experiments on why this works. Some of the planned experiments have already been completed and provide interesting insights.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;At Synthetic Mind, we are pioneering a new way of doing AI research which we call Audience supported research. In addition to free bi monthly articles like this one, we would like to provide more value to our most interested readers. If you would like to support this research, suggest its direction,  get more in depth insights, explore further experiments, or would like to get technical support for repurposing the work produced here for your own research problems you can &lt;a href=&quot;https://gumroad.com/l/bxJfF&quot;&gt;Subscribe&lt;/a&gt;  to our weekly newsletter for a $10 monthly fee.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02877&quot;&gt;Upside Down RL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Synthetic Mind</name><email>your-email@email.com</email></author><category term="Lunar Lander" /><category term="Cartpole" /><category term="Supervised Learning" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/supervisedagents/repo.png" /></entry></feed>